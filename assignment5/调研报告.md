# 创新图像生成方案研究报告：基于层级自适应特征注入的零样本角色一致性生成研究

> **摘要 (Abstract)**
>
> **针对生成式 AI 在叙事创作中面临的“角色一致性”与“动作灵活性”难以兼得的痛点，本文提出了一种名为层级自适应特征注入 (LAFI) 的零样本生成策略。现有方法常因全层特征共享导致参考图的“姿态泄露”，限制了生成角色的动作幅度。**
>
> **本研究利用扩散模型 UNet 架构中“深层表征结构、浅层表征纹理”的层级特性，设计了一种分层注意力路由机制：通过在网络深层阻断信息流以物理隔离参考图的姿态干扰，并在浅层强制注入特征以实现身份纹理的高保真迁移。该方法无需额外训练或微调，旨在从根本上实现身份与姿态的解耦，为漫画绘制及视频生成等长程一致性任务提供了一种高效、灵活的推理期解决方案。**

## 1. 问题定义（Introduction)

### 1.1 背景：从单图生成到叙事一致性的跨越

随着去噪扩散概率模型（Denoising Diffusion Probabilistic Models, DDPM）及其变体（如 Latent Diffusion Models, SDXL, Flux.1）的飞速发展，AI 图像生成领域已经攻克了“高保真度”与“文本语义对齐”两大难关。然而，当 AIGC 的应用场景从生成单张壁纸转向**叙事性内容创作**——如漫画连载、故事分镜绘制、游戏资产设计以及长视频生成时，核心痛点发生了转移。

在这些场景中，用户不再满足于生成一张好看的图，而是要求**同一个角色（Character）**在**一系列图像**中保持严格的身份一致性（Identity Consistency）。这意味着角色的面部特征（五官、发型）、身体特征（体型、肤色）以及服饰细节（纹理、配饰）必须在不同视角、不同光影、甚至大幅度的动作变化下保持恒定。

### 1.2 核心挑战：结构与纹理的纠缠

目前的生成模型在处理“一致性”任务时，面临着一个著名的**“权衡困境” (Stability-Plasticity Dilemma)**：
*   **过度稳定性**：如果强行让模型“记住”参考图，模型往往会连同参考图的**姿态（Pose）**和**构图（Layout）**一起记住。导致生成的新图动作僵硬，无法响应“奔跑”、“跳跃”等高动态的 Prompt 指令。
*   **过度可塑性**：如果允许模型自由发挥动作，模型往往会“忘记”角色的微小细节。例如，参考图中的衣服上有格子花纹，在新动作中可能变成了纯色，或者角色的脸型发生了微妙的漂移。

这种现象的本质原因在于：在扩散模型的潜在空间（Latent Space）中，**“我是谁（Identity）”**的语义信息与**“我在做什么（Structure/Pose）”**的空间信息往往是纠缠在一起的。

### 1.3 研究目标

本研究旨在提出一种**无需训练（Zero-Shot/Training-Free）**的推理期策略，旨在物理层面上实现**“身份纹理”与“几何结构”的解耦**。我们的目标是：给定一张参考图像和一段目标动作描述，生成一张既拥有参考图绝大部分细节特征，又完全遵循目标动作描述的新图像，彻底解决“姿态泄露”问题。

## 2. 相关工作调研 (Related Work)

针对一致性生成问题，2023年至2025年的学术界主要经历了三个技术范式的演变。

### 2.1 基于微调的范式 (Optimization-based Methods)

*   **代表工作**：*DreamBooth (CVPR 2023)*, *LoRA (ICLR 2023)*, *Custom Diffusion (2023)*。
*   **技术原理**：通过在少量（3-5张）参考图上进行反向传播，更新全部或部分模型权重，将特定角色的视觉特征绑定到一个特定的唯一标识符（Unique Identifier，如 "sks dog"）上。
*   **局限性分析**：
    1.  **时间成本**：微调过程通常需要数分钟至数十分钟，无法满足 C 端用户的实时交互需求。
    2.  **过拟合风险 (Overfitting)**：微调后的模型极易陷入“记忆模式”，即死记硬背参考图的背景和姿势。例如，如果参考图是正面照，LoRA 往往难以生成高质量的侧面或背影图。

### 2.2 基于编码器注入的范式 (Encoder-based Injection)

*   **代表工作**：*IP-Adapter (2023)*, *PhotoMaker (CVPR 2024)*, *InstantID (2024)*。
*   **技术原理**：利用预训练的图像编码器（如 CLIP Image Encoder 或 ArcFace）提取参考图的全局语义特征向量，并通过解耦的 Cross-Attention 层将这些特征注入到生成过程中。
*   **局限性分析**：
    1.  **细粒度丢失**：图像编码器通常将整张图压缩为一个或几个向量（Global Embedding），这一过程会不可避免地丢失高频细节。模型能记住“穿红衣服的男人”，但记不住“衣服上有一个破损的纽扣”。
    2.  **特征控制力弱**：作为全局特征注入，难以精确控制特征在画面中的空间分布。

### 2.3 基于注意力共享的范式 (Attention Sharing - Current SOTA)

*   **代表工作**：*MasaCtrl (ICCV 2023)*, *StoryDiffusion (2024)*, *ConsiStory (2024)*。
*   **技术原理**：这是一种推理期干预手段。在生成目标图时，强制将其 Self-Attention 层的 Key ($K$) 和 Value ($V$) 替换或扩展为参考图的 $K$ 和 $V$。其核心假设是：Self-Attention 负责处理图像内部的像素关联，共享它就能共享纹理。
*   **现有痛点 (Research Gap)**：
    目前的 SOTA 方法（如 StoryDiffusion）大多采用**全层共享 (All-Layer Sharing)** 策略。然而，UNet 的深层特征包含高度压缩的空间结构信息。直接共享深层 Attention 会导致参考图的**“姿态泄露” (Pose Leakage)** —— 参考图的站姿骨架会叠加到目标图上，导致目标图出现重影、肢体错位或动作幅度受限。**这是目前 Zero-Shot 一致性生成的最大瓶颈。**

## 3. 解决方法设计 (Method)

为了解决“全层共享”带来的姿态干扰，本报告提出 **Layer-Adaptive Feature Injection (LAFI, 层级自适应特征注入)** 策略。该策略通过**深浅层分离**的机制，利用深度卷积神经网络的归纳偏置 (Inductive Bias)，重新设计了 Attention 的路由机制。

### 3.1 理论基础：UNet 的层级功能解耦

扩散模型的核心 UNet 架构由编码器（Encoder）、瓶颈层（Bottleneck）和解码器（Decoder）组成。不同分辨率的层级承担着截然不同的语义功能：

*   **结构层 (对应 Deep Layers/深层)**：
    *   **位置**：Down-blocks (encoder) 和 Mid-block。
    *   **分辨率**：$64 \times 64 \rightarrow 8 \times 8$。
    *   **功能**：负责生成图像的**低频信息**，包括构图布局、物体形状、骨架姿态和空间透视。
*   **纹理层 (对应 Shallow Layers/浅层)**：
    *   **位置**：Up-blocks (decoder)。
    *   **分辨率**：$16 \times 16 \rightarrow 128 \times 128$。
    *   **功能**：负责生成图像的**高频信息**，包括光影渲染、皮肤纹理、衣物褶皱和五官细节。

**核心假设**：若要在改变动作的同时保持身份，我们应当**阻断深层的信息共享，仅允许浅层的信息流动**。

### 3.2 算法详细设计

本方案不需要训练，直接介入推理过程（Inference Pipeline）。

#### 步骤 1: 双流并行采样 (Dual-Stream Sampling)

我们在同一个 Batch 中并行处理两个去噪任务：

1.  **Reference Stream ($I_{ref}$)**：输入参考图像，添加噪声并进行去噪（或直接通过 Inversion 获得 Latent）。目的是计算并缓存其每层的 $K_{ref}$ 和 $V_{ref}$。
2.  **Target Stream ($I_{tgt}$)**：输入目标 Prompt（例如“a boy running”），生成新图像。

#### 步骤 2: 自适应注意力路由 (Adaptive Attention Routing)

在 Target Stream 计算 Self-Attention 时，也就是计算 $Attention(Q_{tgt}, K, V)$ 时，我们根据当前层级 $l$ 动态修改 $K$ 和 $V$ 的构造方式：

*   **情况 A：当 $l$ 属于深层 (Deep Layers)**
    *   **操作**：**隔离模式 (Isolation Mode)**。
    *   **逻辑**：$K = K_{tgt}, \quad V = V_{tgt}$。
    *   **原理解析**：此时网络正在规划画面的骨架和姿态。我们强制切断与参考图的联系，完全依赖 Target Prompt 生成新的动作结构，从而根除“姿态泄露”。

*   **情况 B：当 $l$ 属于浅层 (Shallow Layers)**
    *   **操作**：**注入模式 (Injection Mode)**。
    *   **逻辑**：我们在序列维度（Sequence Dimension）上拼接特征。
        $$ K = [K_{tgt} \ || \ K_{ref}], \quad V = [V_{tgt} \ || \ V_{ref}] $$
    *   **原理解析**：此时网络正在进行上色和细节填充。通过引入参考图的 $K_{ref}$ 和 $V_{ref}$，Target Stream 的 Query ($Q_{tgt}$) 可以通过注意力机制“查询”到参考图中的纹理细节（如眼睛的形状、衣服的材质），并将其“贴”到深层已经定好的骨架上。

#### 步骤 3: 渐进式时间步控制 (Timestep Scheduling)

扩散过程是从无序（高噪）到有序（低噪）的过程。

*   **前期 (Start -> 80%)**：开启 LAFI 机制，确保身份特征被强力写入。
*   **后期 (80% -> End)**：关闭 LAFI 机制，仅使用自身的 $K_{tgt}, V_{tgt}$。
*   **目的**：让模型在最后阶段利用自身的先验知识对拼接的特征进行平滑处理（Harmonization），消除光照不一致和边缘伪影，使画面更自然。

## 4. 可行性与创新性分析 (Feasibility & Innovation)

### 4.1 方法的可行性分析

1.  **工程实现的预期低门槛**：
    本方案主要涉及对 Attention 计算图的动态干预。在现有的深度学习框架（如 PyTorch）中，理论上可以通过 `register_forward_hook` 或修改 Diffusers 库的 `AttentionProcessor` 类来实现，预计无需编写复杂的底层 CUDA 算子或引入额外的数学变换库，工程实现具有较高的可行性。
2.  **计算资源的低依赖性**：
    相较于需要存储梯度的微调训练（如 LoRA），本方案仅需在推理阶段增加参考图像的前向计算。这意味着在单张消费级显卡（如 RTX 3090 或 4090）上运行 SDXL 级别模型成为可能，有望降低该类技术的硬件准入门槛。
3.  **模型架构的通用性 (Model Agnostic)**：
    该策略利用的是 UNet 架构中普遍存在的层级语义特性，因此不仅适用于 Stable Diffusion 1.5/XL，理论上也有望迁移适配到 Flux.1 或其他基于 DiT (Diffusion Transformer) 架构的模型中，具有一定的泛化潜力。

### 4.2 创新性分析 (Highlights)

1.  **探索“白盒解耦”的优化思路**：
    现有的 StoryDiffusion 等方法倾向于将所有层视为同等重要进行特征复制。本方案尝试深入网络内部的**层级语义分工**，提出了一种基于**结构/纹理物理隔离**的优化思路，为理解和利用扩散模型的内部机制提供了新的视角。
2.  **针对“大动态一致性”问题的尝试**：
    这是本方案的核心设计初衷。通过尝试在深层阻断参考图信息的流动，本方案理论上能够支持**跨度较大**的动作迁移（例如从半身肖像到远景跳水）。这旨在缓解现有 Attention Sharing 方法中常见的姿态泄露问题，探索动作灵活性的边界。
3.  **提供可调节的控制粒度**：
    本方法将“一致性”设计为一个潜在的可调参数。通过调整**注入层的起始位置**，用户或许可以在“保持长相”和“动作变形”之间进行权衡。这种设计思路旨在为创作者提供更灵活的控制自由度。

## 5. 总结 (Conclusion)

本报告针对当前生成式 AI 领域中“多帧角色一致性”这一技术挑战，探讨并提出了一种名为 **Layer-Adaptive Consistency (层级自适应一致性)** 的生成策略设想。

不同于依赖大量计算资源的微调路线或全层特征复制的共享路线，本方案尝试利用扩散模型 UNet 架构内在的 **“深层表征结构，浅层表征纹理”** 这一层级特性。通过设计分阶段、分层级的注意力路由机制，我们期望能够实现：

1.  **身份特征的有效迁移**（主要由浅层注入承担）；
2.  **动作姿态的相对解耦**（主要由深层隔离承担）。

作为一种无需训练数据和额外训练计算资源的推理优化思路，该方法在理论上具备逻辑自洽性。若能在实验中得到验证，它将为漫画创作、故事板设计及 AI 视频生成提供一种高效且灵活的技术参考，具有一定的学术探讨价值和潜在的应用前景。

### 参考文献 (References)

[1] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 10684-10695.
*(注：Latent Diffusion Model 的奠基之作，Stable Diffusion 的基础)*

[2] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 22500-22510.
*(注：对应报告 2.1 节，微调方法的代表)*

[3] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *International Conference on Learning Representations (ICLR)*.
*(注：对应报告 2.1 节，LoRA 的原始论文)*

[4] Ye, H., Zhang, J., Liu, S., Han, X., & Yang, W. (2023). IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models. *arXiv preprint arXiv:2308.06721*.
*(注：对应报告 2.2 节，编码器注入方法的代表)*

[5] Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M. M., & Shan, Y. (2024). PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
*(注：对应报告 2.2 节，针对人脸 ID 的注入优化)*

[6] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., & Zheng, Y. (2023). MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing. *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 22560-22570.
*(注：对应报告 2.3 节，早期 Attention Sharing 的代表)*

[7] Zhou, Y., Zhou, D., Cheng, H., Feng, Z., & Hou, Q. (2024). StoryDiffusion: Consistent Self-Attention for Long-Range Image Generation. *Advances in Neural Information Processing Systems (NeurIPS)*.
*(注：对应报告 2.3 节，解决长程一致性的 SOTA 方法)*

[8] Tewel, Y., Kaduri, O., Gal, R., Kasten, Y., Wolf, L., Chechik, G., & Atzmon, Y. (2024). ConsiStory: Training-Free Consistent Text-to-Image Generation. *arXiv preprint arXiv:2402.03286*.
*(注：对应报告 2.3 节，无需微调的一致性生成)*

[9] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., & Pritch, Y. (2022). Prompt-to-Prompt Image Editing with Cross Attention Control. *International Conference on Learning Representations (ICLR)*.
*(注：Attention 控制技术的理论基础，支撑报告中对 Attention Map 进行干预的可行性)*