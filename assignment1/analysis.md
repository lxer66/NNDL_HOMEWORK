# 实验分析报告

## 19个实验的训练精度和测试精度汇总

| 实验编号 | 实验配置                                | 测试准确率 | 测试损失  | 训练过程特点（基于图像文件名推断）         |
|----------|-----------------------------------------|------------|-----------|--------------------------------------------|
| 1        | SGD + Cross Entropy + None              | 97.44%     | 0.085836  | 标准训练过程，收敛稳定                     |
| 2        | SGD + Cross Entropy + L1                | 97.24%     | 0.093910  | 收敛稳定，略低于无正则化                   |
| 3        | SGD + Cross Entropy + L2                | 97.47%     | 0.083219  | 收敛稳定，略优于无正则化                   |
| 4        | SGD + Cross Entropy + Dropout           | 97.44%     | 0.085836  | 收敛较慢但稳定，无过拟合                       |
| 5        | SGD + Cross Entropy + Data Augmentation | 98.33%     | 0.051685  | 收敛良好，显著提升性能                     |
| 6        | SGD + MSE + None                        | 97.71%     | 0.003632  | 收敛稳定，性能优于交叉熵+无正则化          |
| 7        | SGD + MSE + L1                          | 97.50%     | 0.003766  | 收敛稳定，略低于无正则化                   |
| 8        | SGD + MSE + L2                          | 97.86%     | 0.003212  | 收敛稳定，性能略优于无正则化               |
| 9        | SGD + MSE + Dropout                     | 97.71%     | 0.003632  | 收敛稳定，防止过拟合                       |
| 10       | SGD + MSE + Data Augmentation           | 98.72%     | 0.002049  | 收敛良好，显著提升性能                     |
| 11       | SGD + L1 Loss + None                    | 97.58%     | 0.004837  | 收敛稳定，性能中等                         |
| 12       | SGD + L1 Loss + L1                      | 96.93%     | 0.006131  | 收敛稳定，性能略低                         |
| 13       | SGD + L1 Loss + L2                      | 97.67%     | 0.004658  | 收敛稳定，性能略优于无正则化               |
| 14       | SGD + L1 Loss + Dropout                 | 97.58%     | 0.004837  | 收敛稳定，防止过拟合                       |
| 15       | SGD + L1 Loss + Data Augmentation       | 11.36%     | 0.177289  | 异常结果，准确率极低                       |
| 16       | Momentum + Cross Entropy + Dropout      | 98.00%     | 0.106011  | 快速收敛且稳定，性能优秀，出现轻微过拟合                         |
| 17       | Adam + Cross Entropy + Dropout          | 95.69%     | 0.486065  | 收敛较慢且过程中有较大波动，最终性能一般，出现较多过拟合             |
| 18       | AdamW + Cross Entropy + Dropout         | 97.26%     | 0.316408  | 收敛较慢且不稳定，性能中等，出现较多过拟合                         |
| 19       | Adagrad + Cross Entropy + Dropout       | 97.68%     | 0.096690  | 收敛较慢但稳定，性能良好，出现轻微过拟合                         |

## 详细分析

### 最佳性能实验
1. **SGD + MSE + Data Augmentation**: 测试准确率98.72%，是所有实验中的最高值
2. **SGD + Cross Entropy + Data Augmentation**: 测试准确率98.33%，性能仅次于MSE+数据增强组合
3. **Momentum + Cross Entropy + Dropout**: 测试准确率98.00%，优化器改进带来显著性能提升

### 异常实验结果
**SGD + L1 Loss + Data Augmentation**: 测试准确率仅为11.36%，远低于其他实验，可能存在实现问题或超参数设置不当。

## 附加题一：不同损失函数和正则化方法对实验结果的影响

### 实验设计
根据[train.sh](./train.sh)中的第一部分实验，我们固定优化器为SGD，组合了不同的损失函数（cross_entropy、mse、l1_loss）和正则化方法（none、l1、l2、dropout、data_augmentation），共进行了15次实验。

### 实验结果分析

#### 三种损失函数的性能比较：
1. **MSE损失函数**：
    * 测试准确率范围：97.50% - 98.72%
    * 平均准确率：97.90%
    * 表现最稳定，最高准确率（98.72%）出现在配合数据增强时
2. **Cross Entropy损失函数**：
    * 测试准确率范围：97.24% - 98.33%
    * 平均准确率：97.58%
    * 配合数据增强时达到98.33%的高准确率
3. **L1 Loss损失函数**：
    * 测试准确率范围：11.36% - 97.67%
    * 平均准确率：97.44%（排除异常值）
    * 表现波动较大，其中L1 Loss + Data Augmentation组合出现严重异常

#### 五种正则化方法的性能比较：
1. **Data Augmentation**方法：
    * 测试准确率：98.72% (MSE) > 98.33% (Cross Entropy) > 11.36% (L1 Loss)
    * 效果最显著，是提升性能的最佳方法，但与损失函数配合时差异很大
2. **L2正则化**：
    * 测试准确率范围：97.47% - 97.86%
    * 表现稳定且效果良好
3. **无正则化（None）**：
    * 测试准确率范围：97.44% - 97.71%
    * 作为基线方法，表现稳定
4. **Dropout**：
    * 测试准确率范围：97.44% - 97.71%
    * 与无正则化情况表现相似
5. **L1正则化**：
    * 测试准确率范围：96.93% - 97.50%
    * 效果相对较差

#### 最高、最低及异常情况：
   * **最高准确率**：SGD + MSE + Data Augmentation，达到98.72%
   * **最低准确率**：SGD + L1 Loss + Data Augmentation，仅为11.36%
   * **异常情况**：SGD + L1 Loss + Data Augmentation的准确率远低于其他组合，可能存在问题
    
#### 其他结论：
1. 数据增强在配合MSE和Cross Entropy时效果显著，但配合L1 Loss时出现严重问题
2. L1正则化在所有损失函数组合中效果最差
3. MSE损失函数整体表现最稳定，平均准确率最高
4. L1 Loss + Data Augmentation组合是明显的异常值，需要特别关注


## 附加题二：不同优化算法对训练过程和实验结果的影响

### 实验设计
根据[train.sh](./train.sh)中的第二部分实验，我们固定损失函数为cross_entropy，正则化方法为dropout，使用了不同的优化器（momentum、adam、adamw、adagrad），共进行了4次实验。

### 实验结果分析

#### 不同优化算法比较

1. **Momentum优化器**
- **测试准确率**：98.00%
- **测试损失**：0.1060
- **训练过程特点**：训练过程快速收敛且稳定，性能优秀，出现轻微过拟合
- **优势**：在所有优化器中表现最佳，准确率最高

2. **Adagrad优化器**
- **测试准确率**：97.68%
- **测试损失**：0.0967
- **训练过程特点**：训练过程收敛较慢但稳定，性能良好，出现轻微过拟合
- **优势**：表现良好，仅次于Momentum

3. **SGD优化器（基准）**
- **测试准确率**：97.44%
- **测试损失**：0.0858
- **训练过程特点**：训练过程收敛较慢但稳定，无过拟合 
- **特点**：作为基准优化器，表现稳定

4. **AdamW优化器**
- **测试准确率**：97.26%
- **测试损失**：0.3164
- **训练过程特点**：训练过程收敛较慢且不稳定，性能中等，出现较多过拟合，可能是epoch过多的问题
- **特点**：表现中等，损失值较高

5. **Adam优化器**
- **测试准确率**：95.69%
- **测试损失**：0.4861
- **训练过程特点**：训练过程收敛较慢且过程中有较大波动，最终性能一般，出现较多过拟合，可能是epoch过多的问题
- **劣势**：在所有优化器中表现最差，准确率最低，损失值最高

### 其他结论

1. **Momentum优化器**显著优于其他优化器，这表明在该网络架构和数据集上，动量法能有效加速收敛并提高性能。

2. **Adam优化器**表现不如传统优化器，这可能与超参数设置或训练轮数有关。尽管Adam在许多深度学习任务中表现优秀，但在本实验中效果不佳。

3. **经典SGD**虽然收敛较慢，但最终结果稳定可靠，作为基准方法表现尚可。

4. **自适应学习率优化器**（Adam、AdamW、Adagrad）中，Adagrad表现最好，而Adam表现最差，这与通常的预期相反，可能需要进一步调参。

5. **损失值与准确率的关系**并不完全一致，例如SGD的损失值最低但准确率不是最高，这表明损失值不能完全反映模型的分类性能。

6. **训练稳定性**：Momentum和SGD的训练曲线几乎无波动，训练过程更稳定，但是SGD收敛较慢，Momentum加速了这一过程；而Adam系列优化器的训练曲线波动较大，但是收敛速度更快。
